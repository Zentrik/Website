---
title: Pontryagin's minimum principle
summary: A brief summary of the proof of pontryagin's minimum principle and explanation of $\lambda_0$
author:
- admin
tags: []
categories: []
date: "2021-06-27T17:00:45.785Z" #"2021-06-27T17:00:45.785Z"
lastMod: "2021-06-27T17:00:45.785Z"
featured: false
draft: true

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
image:
  caption: ""
  focal_point: ""

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references 
#   `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []

output:
  blogdown::html_page:
    toc: true
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#proof-summary">Proof Summary</a>
<ul>
<li><a href="#transversality-conditions">Transversality conditions</a></li>
</ul></li>
<li><a href="#nonlinear-optimal-control-theory-by-leonard-david-berkovitz-negash-g.-medhin">Nonlinear Optimal Control Theory by Leonard David Berkovitz, Negash G. Medhin</a></li>
</ul>
</div>

<p>Lagrange problem, minimising <span class="math inline">\(\int_{t_0}^{t_1} g(x, u, t) \,dt\)</span></p>
<p>Bolza problem, minising <span class="math inline">\(h(x(t_1)) + \int_{t_0}^{t_1} g(x, u, t) \,dt\)</span></p>
<p>Mayer problem, minising <span class="math inline">\(h(x(t_1))\)</span></p>
<p>Let <span class="math inline">\(y(t)\)</span> be the state vector and <span class="math inline">\(u(t)\)</span> the control input
<span class="math display">\[\begin{align*}
  \min_{u(t)} \tilde{h}(y(T)) + \int_{0}^{T} \tilde{g}(y(t), u(t)) \,dt \\
  s.t. \ \dot{y} = \tilde{f}(y(t), u(t)), \ y(0) = y^0
\end{align*}\]</span></p>
<p>If we let <span class="math inline">\(\dot{y}_0 = \tilde{g}(x(t), u(t))\)</span> and <span class="math inline">\(y_0 = 0\)</span> then we can construct the optimisation problem as:
<span class="math display">\[\begin{align*}
  &amp;x = [y_0, y^T]^T,\ h(x) = \tilde{h}(y) + y_0,\\
  &amp;f(x(t), u(t)) = [\tilde{g}(y(t), u(t)),\ \tilde{f}(y(t), u(t))^T]^T \\
  &amp;\min_{u(t)} h(x(T)) \\
  s.t. \ &amp;\dot{x} = f(x(t), u(t)), \ x(0) = [0, y^0],\\
\end{align*}\]</span>
Then <span class="math inline">\(H(x, u, \lambda) = \lambda^T f(x, u)\)</span>, <span class="math inline">\(\dot{\lambda}(t) = - \nabla_x H(x^*(t), u^*(t), \lambda(t))\)</span>. Therefore <span class="math inline">\(\dot{\lambda}_0 = 0\)</span> as <span class="math inline">\(y_0\)</span> is not in f(x(t), u(t)). <span class="math inline">\(\lambda(T) = \nabla h(x(T))\)</span> and so if <span class="math inline">\(\lambda_{1 \dots n} = 0\)</span>, WLOG <span class="math inline">\(\lambda_0 = 1\)</span> as <span class="math inline">\(\lambda \neq 0\)</span>. <span class="math inline">\(\lambda_0 = 1, \lambda_{1 \dots n} = 0\)</span> if <span class="math inline">\(x(T)\)</span> is not fixed and <span class="math inline">\(T\)</span> is.</p>
<p><span class="math inline">\(\lambda_0\)</span> = 0 is an abnormal problem, implying the system is not controllable/ the normality condition is not satified/ fixed time, but non fixed final state will never be abnormal? For linear control systems All trajectories for uncontrollable systems are possibly abnormal extremals 
if <span class="math inline">\(\lambda_0 \neq 0\)</span> then we can simply divide by <span class="math inline">\(\lambda_0\)</span> and the resulting hamiltonian will be our standard form.<br />
</p>
<div id="proof-summary" class="section level1">
<h1>Proof Summary</h1>
<p>I will be using <span class="math inline">\(\cdot\)</span> instead of <span class="math inline">\(\langle \rangle\)</span></p>
<p>Free interval optimal control problem: <span class="math inline">\(x(t_0) \in S_{0}\)</span> and <span class="math inline">\(x(t_1) \in S_{1}\)</span>
Fixed interval:</p>
<p>The state at time t is given by <span class="math inline">\(\xi\left( \mu, x_0, t_0, t \right)\)</span></p>
<p>(4.2) We vary the initial state <span class="math inline">\(x_0\)</span>, by setting it to <span class="math inline">\(\gamma(s)\)</span>. We let state with the variation be <span class="math inline">\(\sigma(s, t) = \xi\left( \mu, \gamma(s), t_0, t \right)\)</span> and <span class="math inline">\(v(t) = \sigma&#39;(0, t)\)</span>. Notice <span class="math inline">\(\gamma(s) = \sigma(s, t_0)\)</span> and <span class="math inline">\(\gamma&#39;(0) = v(t_0)\)</span></p>
<p>(4.3)
<span class="math display">\[\begin{align*}
  v(t) &amp;= \boldsymbol{D}_2 \xi\left( \mu, \gamma(0), t_0, t \right) \cdot \gamma&#39;(0) \\
  &amp;= \boldsymbol{D}_2 \xi\left( \mu, \gamma(0), t_0, t \right) \cdot v(t_0) \\
  \dot{v}(t) &amp;= \boldsymbol{D}_4 \boldsymbol{D}_2 \xi\left( \mu, \gamma(0), t_0, t \right) \cdot v(t) \\
  &amp;= \boldsymbol{D}_1 f \left( \xi\left( \mu, \gamma(0), t_0, t \right), \mu(t) \right) \cdot v(t)
\end{align*}\]</span></p>
<p>(4.5) <span class="math inline">\(\lambda(t) \cdot v(t) = \lambda(t_0) \cdot v(t_0)\)</span></p>
<p>(4.6) let <span class="math inline">\(P(t)\)</span> be a hyperplane in state space which is perpindicular to <span class="math inline">\(\lambda(t)\)</span> and <span class="math inline">\(v(t)\)</span> is an element of <span class="math inline">\(P(t)\)</span>. The adjoint equation describes the evolution of a hyperplane along the trajectory <span class="math inline">\(\xi\left( \mu, x_0, t_0, t \right)\)</span>. <span class="math inline">\(\lambda(0) \neq 0\)</span>?.</p>
<p><code>As we shall see, the proof of the Maximum Principle involves approximating the reachable set with convex cones.  In order to generate these approximations we will use specificvariations of controls, called needle variations, that are conjured to exactly give the sort of approximation we need.  There are two sorts of needle variations we will use, depending on whether we are considering the free interval or the fixed interval optimal control problem.</code></p>
<p>(4.8) The fixed interval needle variation is when we switch the input <span class="math inline">\(\mu_\theta\)</span> to <span class="math inline">\(w_\theta\)</span> for a short time period <span class="math inline">\(s \times l_\theta\)</span> ending at <span class="math inline">\(\tau_\theta\)</span> giving <span class="math inline">\(\mu_\theta\left( s, t \right)\)</span>. seen below.
The state at time <span class="math inline">\(\tau_\theta\)</span> following a needle variation is <span class="math inline">\(\xi\left( \mu_\theta\left( s, \cdot \right), x_0, t_0, \tau_\theta \right)\)</span>. The fixed interval needle variation <span class="math inline">\(v_\theta\)</span> is <span class="math inline">\(\left. \frac{d}{d s} \right|_{s=0} \xi\left( \mu_\theta\left( s, \cdot \right), x_0, t_0, \tau_\theta \right)\)</span>, the derivative of the new state with respect to s when the needle variation has no effect.</p>
<p>In (4.9) I think <span class="math inline">\(v_\theta\)</span> is supposed to equal <span class="math inline">\(l_\theta f \left( \xi\left( \mu, x_0, t_0, \tau_\theta \right), \omega_\theta \right) - l_\theta f \left( \xi\left( \mu, x_0, t_0, \tau_\theta \right), \mu \left( \tau_\theta \right) \right)\)</span></p>
<p>(4.10) The set of <span class="math inline">\(v_\theta\)</span> forms a cone when we let <span class="math inline">\(l_\theta\)</span> vary.</p>
<p><code>Now we extend the analysis from the preceding section to allow for the effects of multiple needle variations. The reason for this is simple. The set of fixed interval needle variations at a given Lebesgue point form a cone by Corollary 4.10. However, these will not generally form a convex set, for example, because the control setUneed not be convex (see Exercise E4.2).To generate a convex set of variations from needle variations we allow the times of the needle variations to vary.</code></p>
<p>(4.11) Fixed interval multi-needle variations are the same as before except we have multiple variations and <span class="math inline">\(v_\Theta(t)\)</span> is <span class="math inline">\(\left. \frac{d}{d s} \right|_{s=0} \xi\left( \mu_\Theta\left( s, t \right), x_0, t_0, t \right)\)</span>, the derivative of the new state with respect to s when the needle variation is not in effect at time t.</p>
<p>(4.13) The set of <span class="math inline">\(v_\Theta(t)\)</span> forms a convex cone when we let <span class="math inline">\(l_{\theta, k}\)</span> vary. (clarify what <span class="math inline">\(l_{\theta, k}\)</span> is)</p>
<p><code>Having now discussed a little bit about needle variations,  we  should  really  ask  ourselves  why  these  are  useful  to  look  at. Variations in general provide us with a way to look at trajectories “nearby” a given trajectory. Needle variations do this in a very specific way.  What a needle variation does is isolate the effects of changing the control from its nominal value to a different value around a single instant. The notion of a multi-needle variation encodes the effects of doing this at various differenttimes. Thus the way to think of the set of multi-needle variations is this: It represents the effects at a given time $t$ of instantaneously altering the value of the control around almost all times (specifically at Lebesgue points) preceding it.</code></p>
<p><code>In this chapter we carry out one of the important steps in understanding the Maximum Principle: the elucidation of the relationship between convex cones of multi-needle variationswith the boundary of the reachable set.</code></p>
<p>(5.1) The reachable set is the set of states at <span class="math inline">\(t_1\)</span> that are reached from <span class="math inline">\(x_0\)</span> at <span class="math inline">\(t_0\)</span> if the input can be any element in the input set.</p>
<p>(5.2) We calculate the evolution of <span class="math inline">\(v_\theta\)</span> along the trajectory <span class="math inline">\(\xi\left( \mu, x_0, t_0, t \right)\)</span> giving <span class="math inline">\(x = \left. \frac{d}{d s} \right|_{s=0} \xi\left( \mu_\theta\left( s, \cdot \right), x_0, t_0, t \right)\)</span>. This gives us the derivative of the state at time <span class="math inline">\(t\)</span> when the needle variation at time <span class="math inline">\(\tau_\theta\)</span> is introduced. The combination of all <span class="math inline">\(x\)</span> where <span class="math inline">\(\tau_\theta\)</span> and <span class="math inline">\(l_\theta\)</span> take on all feasible values. ‘The idea of the fixed interval tangent cone is that it should be a set of “directions” fromwhich trajectories of the system emanate.’</p>
<p>(5.3) Describes the coned convex hull of <span class="math inline">\(v_\Theta\)</span>. Each <span class="math inline">\(\Theta\)</span> has <span class="math inline">\(k_a\)</span> <span class="math inline">\(\theta\)</span> in it, each with a different <span class="math inline">\(\tau, l\)</span> and <span class="math inline">\(\omega\)</span>. This gives a fixed interval tangent r-simplex cone.</p>
<p>(5.4) All directions in the fixed interval tangent cone that are contained in the fixed interval tangent r-simplex cone are generated by a fixed interval multi-needle variation.
<code>The point is that all directions in K(μ,x0,t0,t) that are contained in fixed interval tangent simplex cones are generated by a fixed interval multi-needle variation. The non obvious thing is that all interior points in K(μ,x0,t0,t) have this property. The following result records this, along with another useful characterisation of the fixed interval tangent cone.</code></p>
<p>(5.5) ‘The fixed interval tangent cone is equal to the
Note that any fixed interval tangent simplex cone at timetis, by definition,contained in the fixed interval tangent cone. Thus the closure of the set of all fixed interval tangent simplex cones at time <span class="math inline">\(t\)</span> is contained in the fixed interval tangent cone. since the latter is closed.’</p>
<p>(5.10) Points interior to fixed interval tangent cones are in the reachable set. Why? because we can let s = 1 and just scale l appropriately as sl is what matters for the needle variation. and so by 4.9 this is in reachable set? </p>
<p>(5.12) We have <span class="math inline">\(p \cdot f(x,u) \leq p \cdot f(x, u^*)\)</span> and let <span class="math inline">\(F_\sigma(x) = f(x, u)\)</span>. The set <span class="math inline">\(F_\sigma(x)\)</span> lies on one side of the hyperplane <span class="math inline">\(p \cdot f(x,u) \leq p \cdot f(x, u^*)\)</span>. Also the hamiltonian has the point <span class="math inline">\(p \cdot f(x, u^*)\)</span> on it so support hyperplane. We are considering <span class="math inline">\(\mathbb{R}^n\)</span>, the output space of <span class="math inline">\(f(x, u)\)</span>, so <span class="math inline">\(F_\sigma(x)\)</span> <span class="math inline">\(f(x, u^*)\)</span> lies in the set and has to be on boundary as it is the max of <span class="math inline">\(p \cdot f(x, u)\)</span> over u. ?</p>
<p>(5.13)
We let <span class="math inline">\(l_\theta = 1 \ \therefore \ v_\theta = f\left( \xi, \omega \right) - f\left( \xi, \mu(t) \right)\)</span> by (4.9). <span class="math inline">\(v_\theta\)</span> is in the fixed interval tangent cone and the evolution till time <span class="math inline">\(\tau\)</span> is in <span class="math inline">\(K(\mu, x_0, t_0, \tau)\)</span>.</p>
<p>So by (4.7) we can evovle the <span class="math inline">\(\lambda\)</span> instead giving hamiltonian pointwise maximum.</p>
<p>We have <span class="math inline">\(\lambda(t) \cdot v_\theta \leq 0\)</span>, therefore <span class="math inline">\(\left. \frac{d}{d s} \right|_{s=0}H\)</span> is -ve?.</p>
<p>The existence of a support hyperplane or the free interval tangent cone almost everywhere, says that applying needle variations the trajectories can only move in “one way”.’ Lemmata 5.12and 5.13 say that this implies that the control must also have an “extremal” property atalmost all instants precedingτ. (The preceding discussion really only has poignancy whenthe free interval tangent cone has a nonempty interior.)’</p>
<p>We have <span class="math inline">\(\lambda(t) \cdot v_\theta \leq 0\)</span> and <span class="math inline">\(v_\theta = 0\)</span>, so we have a supporting hyperplane with the fixed interval tangent cone. This implies the maximisation of the Hamiltonian??</p>
<p>In 5.13 We assume that <span class="math inline">\(\lambda(\tau) \cdot v \hspace{0.5cm} \forall \ v \in K_\tau\)</span> where <span class="math inline">\(K(\mu, x_0, t_0, t) \subset K_t \subset \mathbb{R}^n\)</span> for each <span class="math inline">\(t \in [t_0, t_1]\)</span> and that <span class="math inline">\(\lambda\)</span> is the adjoint response with <span class="math inline">\(\lambda(\tau)\)</span> at time <span class="math inline">\(\tau\)</span>. This implies that <span class="math inline">\(\mu(t)\)</span> maximises the hamiltonian.</p>
<p><span class="math inline">\(K_t\)</span> is a convex cone so it includes the origin. So <span class="math inline">\(\lambda(\tau) \cdot v = 0\)</span> is a supporting hyperplane to <span class="math inline">\(K_\tau\)</span>.</p>
<p>(5.14) If <span class="math inline">\(span \left( f(\xi(\mu, x_0, t_0, \tau), \mu(\tau)) \right) \subset K_\tau\)</span> where <span class="math inline">\(K_\tau \subset \mathbb{R}^n\)</span> is a convex cone and <span class="math inline">\(\lambda(t) \cdot v \leq 0 \ \forall v \in K_\tau\)</span> then Hamiltonian is 0 for all inputs?</p>
<p>(5.15) If control is bounded then Hamiltonian is a constant wrt time.</p>
<p>(5.16) If <span class="math inline">\(ξ(μ^∗,x_0,t_0,t_1) ∈ bd(\mathcal{R}(x_0,t_0,t_1))\)</span> then there is a hyperplane <span class="math inline">\(P(t_1)\)</span> seperating <span class="math inline">\(v_0\)</span> and <span class="math inline">\(K(\mu, x_0, t_0, t_1)\)</span>. If we let <span class="math inline">\(\lambda_*(t_1)\)</span> be orthogonal to <span class="math inline">\(P(t_1)\)</span> and contained in a half-space not containing <span class="math inline">\(K(\mu, x_0, t_0, t_1)\)</span> and let it be the adjoint response taking <span class="math inline">\(\lambda_*(t_1)\)</span> at time <span class="math inline">\(t_1\)</span> then lemmata 5.13 and 5.15 follows. So pointwise maximum is satisfied.</p>
<p>(5.17) Trajectories in the interior of the fixed time reachable setremain in the interior</p>
<p>(6.2) <span class="math inline">\(\xi_*^0(t_1)\)</span> has to be the minimum value it can be for <span class="math inline">\(\xi_*(t_1)\)</span> in the reachable set, otherwise it couldn’t be the optimal solution as it doesn’t minimise the cost. Therefore, <span class="math inline">\(\hat{\xi}\)</span> has to lie on the boundary of the reachable set of the extended set.</p>
<p>(6.3) <span class="math inline">\((-1, 0)\)</span> cannot lie in the interior of <span class="math inline">\(\hat{K}(\mu_*, \hat{x}_0, t_0, t_1)\)</span>, as it is a cone so if <span class="math inline">\((-1, 0)\)</span> lies in the cone that would imply that the reachable set includes points with a lower <span class="math inline">\(\xi^0(t_1)\)</span> value and <span class="math inline">\(\xi(t_1)\)</span> which contradicts (6.2). Therefore, there exists a hyperplane <span class="math inline">\(\hat{P}(t_1)\)</span> such that <span class="math inline">\((-1, 0)\)</span> is contained in one of the closed half-spaces defined by <span class="math inline">\(\hat{P}(t_1)\)</span> and <span class="math inline">\(\hat{K}(\mu_*, \hat{x}_0, t_0, t_1)\)</span> is contained in the other closed half-space. We take <span class="math inline">\(\hat{\lambda}_*(t_1)\)</span> to be a vector orthorgonal to <span class="math inline">\(\hat{P}(t_1)\)</span> and contained in the the half-space containing <span class="math inline">\((-1, 0)\)</span>. Then <span class="math inline">\(\hat{\lambda}_*(t_1) \cdot (-1, 0) \geq 0\)</span> as both are in the same halfspace and <span class="math inline">\(\hat{\lambda}_*(t_1) \cdot \hat{v} \leq 0, \hspace{0.5cm} \hat{v} \in \hat{K}(\mu_*, \hat{x}_0, t_0, t_1)\)</span>. <span class="math inline">\(\hat{\lambda}_*(t_1) \cdot (-1, 0) \geq 0 \implies \hat{\lambda}_*(t_1) \leq 0\)</span>. We then define <span class="math inline">\(\hat{\lambda}_*\)</span> to be the adjoint response equal to <span class="math inline">\(\hat{\lambda}_*(t_1)\)</span> at time <span class="math inline">\(t_1\)</span>. From the equations for the adjoint response we immediately have <span class="math inline">\(\dot{\lambda}_*^0(t) = 0\)</span> (since <span class="math inline">\(\hat{f}\)</span> is independent of <span class="math inline">\(x^0\)</span>) and so <span class="math inline">\(\lambda_*^0\)</span> is constant and nonpositive. If <span class="math inline">\(\lambda_*^0 \neq 0\)</span> then we can define <span class="math inline">\(\lambda_*^0(t_1) = -1\)</span> (as the <span class="math inline">\(\lambda\)</span> can be scaled whilst still being orthogonal and in the same closed half-space) and so <span class="math inline">\(\lambda_*^0 = -1\)</span>. Alternatively divide <span class="math inline">\(\Lambda\)</span> by <span class="math inline">\(\lambda^0\)</span> and the differential equation will still be satisfied.</p>
<p>The theorem then follows from lemmata 5.13 and 5.15.</p>
<p>WHY <span class="math inline">\(\lambda_0 \neq 0\)</span> for every <span class="math inline">\(t \in [t_0, t_1]\)</span> IDK. <span class="math inline">\(\lambda \neq 0\)</span> as it is orthogonal to hyperlane, <span class="math inline">\(\therefore\)</span> cannot be <span class="math inline">\(\boldsymbol 0\)</span></p>
<p>(6.4) If control is bounded then Hamiltonian is a constant wrt time.</p>
<p>() We approximate the reachable set with convex cones. The reachable set is the states we can get to from <span class="math inline">\(x_0\)</span> at <span class="math inline">\(t_0\)</span> at <span class="math inline">\(t_1\)</span> by setting the input. The needle variations form a convex cone, which is a subset of the reachable set. ‘The idea of the fixed interval tangent cone is that it should be a set of “directions” from which trajectories of the system emanate.’</p>
<div id="transversality-conditions" class="section level3">
<h3>Transversality conditions</h3>
<p>Before we stated that we should not be able to stay at the same state and decrease cost. Here we will be using the fact that we cannot stay in the stay in the state set <span class="math inline">\(S_0\)</span> and <span class="math inline">\(S_1\)</span> whilst decreasing cost.</p>
<p>(6.5) Fig 6.2, 6.3 are quite helpful here. <span class="math inline">\(U\)</span> is a ball, where in the last dimension it is non negative (i.e. the ball has been cut in half). More precisely <span class="math inline">\(U\)</span> contains the semi ball? <span class="math inline">\(\phi\)</span> is a continuous mapping of <span class="math inline">\(U\)</span> which has a continuous inverse. <span class="math inline">\(D \phi(y)\)</span> has one unique output for each <span class="math inline">\(y \in U\)</span> (as <span class="math inline">\(U \subset U&#39;\)</span>, I have replaced the references to <span class="math inline">\(U&#39;\)</span> in this line. is this valid?). <span class="math inline">\(\epsilon = \phi(U)\)</span> and the boundary of <span class="math inline">\(\epsilon\)</span> is just <span class="math inline">\(\phi(y)\)</span> where <span class="math inline">\(y\)</span> are points on the flat part of the semi ball, <span class="math inline">\(U\)</span> (points where the last dimension is 0).</p>
<p>(6.6) We denote <span class="math inline">\(T_x S_0 = \ker(\nabla \Phi_0(x))\)</span> the tangent space to <span class="math inline">\(S_0\)</span> at a point <span class="math inline">\(x \in S_0\)</span>. Now if we have a starting state <span class="math inline">\(x_0 \in S_0\)</span>, the extended state (including cost) is <span class="math inline">\((0, x_0)\)</span>. Let the state at time t starting at the extended state, with control <span class="math inline">\(\mu(t)\)</span> be <span class="math inline">\(\xi(\mu, (0, x_0), t_0, t)\)</span> which we will denote as <span class="math inline">\(\xi(t)\)</span> for brevity. We have a needle variation with <span class="math inline">\(\tau \in (t_0, t_1)\)</span>. We denote <span class="math inline">\(\mathscr{K}(\mu, x_0, t_0, t) = \text{cl}(\text{conv cone} (\Phi(\boldsymbol \mu, x_0, t_0, t_0, t)((\boldsymbol T_{\xi(t_0)} S_0) \cup K(\mu, x_0, t_0, t)))\)</span>. <span class="math inline">\((\boldsymbol \Phi(\mu, x_0, t_0, t_0, t)((\boldsymbol T_{\xi(t_0)} S_0)\)</span>, recall that <span class="math inline">\(\Phi\)</span> is a map from of a state between two times in this case from <span class="math inline">\(t_0\)</span> to <span class="math inline">\(t\)</span>?, so this is the tangent space at time <span class="math inline">\(t\)</span> as the system has evolved according to the state dynamics <span class="math inline">\(f\)</span>?
The union gives us</p>
<p>(6.7) Let <span class="math inline">\(\varepsilon\)</span> be an edged set with <span class="math inline">\(\xi(\mu, x_0, t_0, \tau)\)</span> on its boundary. If the tangent half-space of <span class="math inline">\(\varepsilon\)</span> at <span class="math inline">\(\xi(\mu, x_0, t_0, \tau)\)</span> and the cone <span class="math inline">\(\mathscr{K}(\mu, x_0, t_0, \tau)\)</span> are not seperable, then there exists <span class="math inline">\(x&#39;_0 \in S_0\)</span> such that there is an element of <span class="math inline">\(\mathcal{R}(x&#39;_0, t_0, t_1)\)</span> in the interior of <span class="math inline">\(\varepsilon\)</span>.</p>
<p>I also use Geometric Approach to Pontryagin’s Maximum Principle to explain the transversality conditions as it is clearer in places.
Imagine we have a needle variation as before but now we also vary the initial state, the smallest closed convex cone containing these variations is <span class="math inline">\(\mathscr{K}\)</span>. Now if we define a . Here we define a manifold <span class="math inline">\(M_f\)</span> that is equivalent to <span class="math inline">\(\hat{S}_1\)</span></p>
<p>We define an edged set <span class="math inline">\(\hat{S}_1\)</span> which is the set of all states which lie in <span class="math inline">\(S_1\)</span> and whose cost, <span class="math inline">\(x_0\)</span>, is less than or equal to the optimal final cost. We also define an edged set <span class="math inline">\(\hat{S_\tau}\)</span> which is the set of states at time <span class="math inline">\(\tau\)</span> which follow the state dynamics under the optimal control trajectory and which lie in <span class="math inline">\(\hat{S}_1\)</span> at <span class="math inline">\(t_1\)</span>. The tangent halfspace to <span class="math inline">\(\hat{S}_1\)</span> at <span class="math inline">\(\hat{\xi}_*(t_1)\)</span> is the convex cone of the union between <span class="math inline">\((-1, 0)\)</span> and <span class="math inline">\(\hat{T}_{\hat{\xi}_*(t_1)} \hat{S}_1\)</span>. …</p>
<p>(6.9) The cones <span class="math inline">\(\hat{\mathscr{K}}(\mu_*, \hat{x_0}, t_0, t_1)\)</span> and <span class="math inline">\(\hat{T}_{\hat{\xi}_*(t_1)} \hat{S}_1\)</span> are seperable. Assume that they are not, as <span class="math inline">\(\hat{\mathscr{K}}(\mu_*, \hat{x_0}, t_0, t_1) = \cup_{t \in (t_0, t_1)} \hat{\mathscr{K}}(\mu_*, \hat{x_0}, t_0, t)\)</span> (explain why) then there exists a <span class="math inline">\(\tau\)</span> such that <span class="math inline">\(\hat{\Phi}(\mu_*, \hat{x_0}, t_0, \tau, t_1)(\hat{\mathscr{K}}(\mu_*, \hat{x_0}, t_0, \tau))\)</span> and <span class="math inline">\(T^+_{\hat{\xi}_* (t_1)} \hat{S}_1\)</span> are not seperable (WHY). The latter cone is simply <span class="math inline">\(T^+_{\hat{\xi}_* (\tau)} S_\tau\)</span>, so using lemma 6.7 we conclude there is a control …(essentially copy paste) so not optimal.</p>
<p>By lemma 6.9 we know that the cones <span class="math inline">\(\hat{\mathscr{K}}(\mu_*, \hat{x_0}, t_0, \tau)\)</span> and <span class="math inline">\(T^+_{\hat{\xi}_* (t_1)} \hat{S}_1\)</span> are seperable. So there exists a <span class="math inline">\(\hat{\sigma}\)</span> such that <span class="math inline">\(\hat{\sigma} \cdot \hat{v} \leq 0\)</span> for all <span class="math inline">\(\hat{v} \in \hat{\mathscr{K}}(\mu_*, \hat{x_0}, t_0, t_1)\)</span> and <span class="math inline">\(\hat{\sigma} \cdot \hat{v} \geq 0\)</span> for all <span class="math inline">\(\hat{v} \in T^+_{\hat{\xi}_* (t_1)} \hat{S}_1\)</span>.</p>
<p>As <span class="math inline">\(\hat{K}(\mu_*, \hat{x}_0, t_0, t_1) \subset \hat{\mathscr{K}}(\mu_*, \hat{x}_0, t_0, t_1)\)</span> and <span class="math inline">\((-1, 0) \in T^+_{\hat{\xi}_* (t_1)} \hat{S}_1\)</span>, then <span class="math inline">\(\sigma\)</span> satisfies the condtion of the adjoint … So we can let <span class="math inline">\(\lambda_*(t_1) = \sigma\)</span>?
As <span class="math inline">\(\hat{T}_{\hat{\xi}(t_1)} S_1 \subset T^+_{\hat{\xi}_*(t_1)} \hat{S}_1\)</span> and <span class="math inline">\(\hat{T}_{\hat{\xi}(t_1)} S_1 = \{(0, v) | v \in T_x S_1 \}\)</span>. So <span class="math inline">\(\sigma(t_1) \cdot v \geq 0\)</span> for all <span class="math inline">\(v \in T_{\xi_*(t_1)} S_1\)</span>. As <span class="math inline">\(T_{\xi_*(t_1)} S_1\)</span> is a subspace, for any <span class="math inline">\(v \in T_{\xi_*(t_1)} S_1\)</span> there is a <span class="math inline">\(-v \in T_{\xi_*(t_1)} S_1\)</span>. So <span class="math inline">\(\sigma(t_1) \cdot v \geq 0\)</span> and <span class="math inline">\(\sigma(t_1) \cdot -v \geq 0\)</span>, therefore <span class="math inline">\(\sigma(t_1) \cdot v = 0\)</span> for all <span class="math inline">\(v \in T_{\xi_*(t_1)} S_1\)</span>. As <span class="math inline">\(\sigma \neq 0\)</span> then <span class="math inline">\(\sigma\)</span> is orthogonal to <span class="math inline">\(T_{\xi_*(t_1)} S_1\)</span>, so <span class="math inline">\(\lambda_*(t_1)\)</span> is orthogonal to <span class="math inline">\(T_{\xi_*(t_1)} S_1\)</span>.</p>
<p>(7.3)
(7.4)</p>
<p>(4.9) ξ(μΘ(s,·),x0,t0,τ_) =ξ(μ,x0,t0,τ1) +sv_θ + o(s)
(4.12) ξ(μΘ(s,·),x0,t0,t) =ξ(μ,x0,t0,t) +sΦ(μ,x0,t0,τ1,t)·vθ1+…+sΦ(μ,x0,t0,τk,t)·v_θk + o(s)</p>
</div>
</div>
<div id="nonlinear-optimal-control-theory-by-leonard-david-berkovitz-negash-g.-medhin" class="section level1">
<h1>Nonlinear Optimal Control Theory by Leonard David Berkovitz, Negash G. Medhin</h1>
<p>The sequel to citation 23 in the paper</p>
<p>I will be using the same notation as used in the previous section. <span class="math inline">\(e(\phi) = (t_0, \xi(0), t_1, \xi(t_1))\)</span>, the set <span class="math inline">\(\mathcal{B}\)</span> of points <span class="math inline">\(e(\phi)\)</span> in <span class="math inline">\(\mathbb{R}^{2n + 2}\)</span>. <span class="math inline">\(\mathcal{B} = \{ (t_0, \xi_0, t_1, \xi_1) : (t_0, \xi_0) \in \mathcal{T}_0, \ (t_1, \xi_1) \in \mathcal{T}_1 \}\)</span>.  is similar to S in previous section but also includes constraint on the time.</p>
<p>If <span class="math inline">\((t_0, x_0)\)</span> are fixed and we have a n-dimensional manifold <span class="math inline">\(\mathcal{T}_1\)</span>. And if the trajectory <span class="math inline">\(\xi\)</span> is not tangent to <span class="math inline">\(\mathcal{T}_1\)</span> at <span class="math inline">\((t_1, \xi(t_1))\)</span> then <span class="math inline">\(\lambda^0 \neq 0\)</span></p>
<p>6.3.30
a) If <span class="math inline">\(\lambda \neq 0\)</span> let <span class="math inline">\(\boldsymbol \lambda^0 = -1\)</span>, <span class="math inline">\(\boldsymbol \lambda = \frac{\lambda}{|\lambda^0|}\)</span> and so <span class="math inline">\(\boldsymbol H = \frac{H}{|\lambda^0|} = \hat{\boldsymbol \lambda} \cdot f\)</span>. <span class="math inline">\(\dot{\boldsymbol \lambda} = - \frac{1}{|\lambda^0|} \nabla_\xi H = - \nabla_\xi \boldsymbol H\)</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Let <span class="math inline">\(\mathcal{T}_0\)</span> be a point and <span class="math inline">\(\mathcal{T}_1\)</span> be a n-dimensional manifold of class C^(1) (the set of all differentiable functions with a continouous first derivative). If an extremal trajectory is not tangent to <span class="math inline">\(\mathcal{T}_1\)</span> then <span class="math inline">\(\lambda^0 \neq 0\)</span>. WHY?</li>
</ol>
<p>We are minisiming <span class="math display">\[\begin{align*}
  \gamma(e(\phi)) + \int_{t_0}^{t_1} A \dot{x} \cdot \dot{x} + B \dot{x} \cdot a(t, x) + b(t, x) \,dt
\end{align*}\]</span>
Assume initial and terminal times are fixed. Let <span class="math inline">\(t_0 = 0\)</span> and <span class="math inline">\(t_1 = 1\)</span> for typographic simplicity. The set <span class="math inline">\(\mathcal{B}\)</span> consists of points <span class="math inline">\((0, x_0, 1, x_1)\)</span>.</p>
<p>(7.4.8) Let {} minimise the cost function over the set of functions that start <span class="math inline">\(\epsilon\)</span> close to the initial state and whose state derivative stay <span class="math inline">\(\epsilon\)</span> close throughout the trajectory?. eq (7.4.7). Assumption (7.4.1) v) gives that the endpoint <span class="math inline">\(e(\phi) \in \mathcal{B}\)</span> and is absolutely continouous and has derivative for all time?</p>
<p>Perhaps, final cone has to be contained in <span class="math inline">\(S_1\)</span>?. So if we can construct seperating hyperplane to <span class="math inline">\(S_1\)</span> and <span class="math inline">\((0, -1)\)</span> we have done it for cone. So if we let <span class="math inline">\(S_1\)</span> be contained in the plane/ parallel to it, then <span class="math inline">\(\lambda(t_1)\)</span> can be perpindicular to <span class="math inline">\(S_1\)</span>. To get to <span class="math inline">\(\mathcal{T}\)</span> add a new state which represents time and then <span class="math inline">\(\mathcal{T}\)</span> can be represented as a state constraint.</p>
<p>We can get the transversality conditions from 7.11, convert the variable time problem to a fixed time problem and include time in the state.</p>
<p>‘Apart from the transversality conditions, the main difference between FPMP and PMP is thefact that the domain of the curves in the optimal control problems is unknown. That introducesa new necessary condition: the supremum of the Hamiltonian must be zero, not just constant.’ Geometric_Approach_to_Pontryagins_Maximum_Princip.pdf
Nice article: <a href="https://www.janheiland.de/post/pontr-pendl/#the-maximum-principle" class="uri">https://www.janheiland.de/post/pontr-pendl/#the-maximum-principle</a>
<a href="https://encyclopediaofmath.org/wiki/Bolza_problem" class="uri">https://encyclopediaofmath.org/wiki/Bolza_problem</a></p>
<p><a href="https://math.stackexchange.com/questions/3898940/tangent-vectors-as-directional-derivatives-on-manifolds" class="uri">https://math.stackexchange.com/questions/3898940/tangent-vectors-as-directional-derivatives-on-manifolds</a>
<a href="https://tutorial.math.lamar.edu/Classes/CalcIII/TangentNormalVectors.aspx" class="uri">https://tutorial.math.lamar.edu/Classes/CalcIII/TangentNormalVectors.aspx</a></p>
<p>We have a parametric curve, <span class="math inline">\(r(t)\)</span>. The tangent is <span class="math inline">\(\left. \frac{d}{d t} \right|_{t=a}r(t)\)</span> at a.
If we have a function <span class="math inline">\(f(x)\)</span> we can let <span class="math inline">\(r(t) = f(a + t v)\)</span> (v is the direction we are making our nudge in). Then the tangent is <span class="math inline">\(\left. \frac{d}{d t} \right|_{t=a}r(t) = D_v f(a)\)</span> at a.
<a href="https://www.youtube.com/watch?v=cHNT7_F8m1Y&amp;list=PLSQl0a2vh4HC5feHa6Rc5c0wbRTx56nF7&amp;index=75" class="uri">https://www.youtube.com/watch?v=cHNT7_F8m1Y&amp;list=PLSQl0a2vh4HC5feHa6Rc5c0wbRTx56nF7&amp;index=75</a></p>
<p>WHY ARE NEEDLE VARIATIONS USED AND NOT SOMETHING ELSE.</p>
</div>
